# ğŸ§  AI Worker â€” LLaMA Integration

This repository contains the integration of a **LLaMA-based Large Language Model (LLM)** with a **YOLOv7 object detection** pipeline for real-time decision making.  
The system allows the LLM to interpret object detection results (e.g., â€œCamera detected: plastic bottleâ€) and map them to task-specific robot actions â€” such as assigning the correct robotic arm for sorting.

---

## âš™ï¸ Installation

1ï¸âƒ£ Clone this repository 
```bash
git clone https://github.com/Kodi09/AI-worker---LLama-integration.git  
cd AI-worker---LLama-integration  
```
---

2ï¸âƒ£ Install dependencies  
```bash
pip install -r requirements.txt  
```
Download YOLOv7 (if not already included):  
```bash
git clone https://github.com/WongKinYiu/yolov7.git  
```
---
3ï¸âƒ£ Install Ollama

Ollama lets you run LLaMA models locally (no internet or API key needed).
Download it from the official site for your platform: ğŸ”— https://ollama.ai/download.
After installing, verify itâ€™s working:
```bash
ollama --version
```
---
4ï¸âƒ£ Start Ollama (for LLaMA)  
```bash
ollama serve  
ollama run llama3  
```
---

5ï¸âƒ£ Run the integration  
```bash
python LLM+Yolo.py  
```
---

## ğŸ§  Example Workflow
Camera â†’ YOLOv7 Detection â†’ LLaMA Reasoning â†’ Robot Arm Assignment  

1. YOLO detects: plastic bottle  
2. LLaMA processes text prompt: â€œCamera detected: plastic bottleâ€  
3. LLM returns decision: â€œAssign to Robot Arm 2 (plastic)â€  
4. Robot control module executes the corresponding movement.   

---

## ğŸ™Œ Credits
YOLOv7 by WongKinYiu  
Project developed at NYU Robotics Lab as part of AI agent autonomous sorting research.  
